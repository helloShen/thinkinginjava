---
layout: post
title: "Thinking in Java 读书笔记：第十一章 - 容器"
chapter: "Chapter 11"
description: >
  本书的容器分成两章来讲：第11章是初级部分，第17章是高级内容。本章初级内容，作者主要是介绍了API。List, Set, Queue, Map一个一个讲过来。然后还介绍了他们的接口Collection，Map等。一个很重要的知识点是“迭代器(Iterator)”。所有Collection接口下的容器都实现了Iterable接口。可以使用foreach语法糖。然后在第17章进阶部分，作者进一步深入探讨了容器类的源码。
---



### Array不是一般意义上的“容器”

讲容器之前，请允许我先把Array和其他所有的容器区分开来。它是Java语言内置(compiler-supported type)的数据类型，它是**唯一可以存放基本型的容器**。而且Array非常地高效。
这里有一篇好文章：[**《历史上的 Collection 类 ― 数组》**](http://www.ibm.com/developerworks/cn/java/j-arrays/)，可以帮助我们了解Array的基本情况。

Array为什么效率高呢？这要从Array本身在内存中的结构说起（下图）：
![ArrayMemory1](/thinkinginjava/uploads/tij4-11/ArrayMemory1.jpg)
对于Array的具体实现，有下面几个重要事实：

* Array是一个对象**Object**

* Array在Stack栈区只占一个“**long**”型，储存指向在Heap堆区的实际数据首地址的**引用**

* Array在Heap堆区的具体数据，所占的是“**连续空间**”

![ArrayMemory2](/thinkinginjava/uploads/tij4-11/ArrayMemory2.jpg)
如上图，多维数组的实现方式，是靠一个“引用的数组”来过度的。第一层数组的每一个地址，只储存指向具体数据数组的一个引用。

Array效率高的秘密，就在于它在内存中占据的空间是连续的。因此如下图，只要掌握了在内存中的首地址，以及数组的长度，对于任意想要的元素，数组只要在首地址基础上加上偏移值，就能找到该元素。而对内存的操作，seek偏移值的开销是非常小的。
![ArrayMemory2](/thinkinginjava/uploads/tij4-11/ArrayMemory3.gif)

但连续空间是把双刃剑，副作用是Array**声明的时候必须固定长度，之后不能修改**。这极大地限制了Array的使用，因为大多数的使用场景，声明数据的时候，并不知道我们需要多大的容器。这才导致了后面其他容器的出现。


```java


int[] myArray = new int[10];


```



由于Java类库已经经历了近20年的改进和打磨，java容器的效率已经有了明显的提升，尤其随着应用场景的复杂化，说Array的效率完胜容器已经不合时宜了。对一些常用的操作，比如说批量插入，或者遍历，容器提供的现成方法的效率，已经可以完胜我们自己用Array慢慢瞎捣鼓的代码了。更不用说对于更复杂的比如说并发应用。自己动手造轮子的时代已经过去了。



### java.util类库中的容器

先声明，这一章只负责告诉读者一些关于util类库定义的容器的一些有用的结论，来帮助我们在编程的时候尽量选择正确合适的容器。至于容器内部的一些算法细节，会在之后的《容器深谈》这一章中探讨。

Java类库java.util提供的容器到底有哪些？下面这张图略简陋，但有助于整理思路。
![container](/thinkinginjava/uploads/tij4-11/container.png)



#### 容器两大分支：Collection和Map

uitl类库容器的共同特征是：**动态调节长度**。为了弥补Array固定长度的遗憾。
但鱼和熊掌不可兼得。实现动态扩展数组长度，必然就要牺牲效率，为它们的弹性付出代价。因此它们的效率都不如Array。但它们之所以存在，除了动态长度外，都具备各自特色的功能，各有所长。

总的来说，容器分为两大类，都以接口形式存在，

1. **Collection** 指的是一组元素的集合。强调的是元素本身。
2. **Map** 的特殊功用在于它为每个元素维护的一组“KEY-VALUE”对。Key用来搜索value值。典型的应用就是“词典”。




#### Collection的三大将

它们也都是接口，而且各有所长，

1. **List**：最正统。强调“**一组元素的序列**”。通常是元素插入集合时候的顺序。
2. **Set**：的特点是其中**元素的唯一性**。插入新元素前会检查已有列表，如果已经有了，就不会再插入。
3. **Queue**：虽然也强调的是序列，但更主要的是“**获取下一个元素的顺序**”。最简单的一种就是“FIFO(First In First Out)”先进先出。



#### 关于List

事实上最常用的List只需要记住两种，

1. ArrayList
2. LinkedList

剩下一个**Vector**被作者说成是过时的容器，不推荐使用。而另一个**CopyOnWriteList**是用于并发系统，确保多线程访问时数据同步安全性的，不属于常规使用。

ArrayList和LinkedList它们俩个经常被拿来比较性能。互为对方一生的羁绊。关于它们俩，需要记住的重要事实是，

1. ArrayList的实现基于Array。**精髓在于：随机访问get(int index)方法。复杂度是常数**O(1)。** 但缺点是:add(int index, <E> element)，remove(int index)的时候要逐个前移或后移操作位后面的所有元素，所以复杂度是O(n)。
2. **LinkedList的精髓在于同时实现了Queue的接口。** 在Queue只是个接口，以及Java官方枪毙Stack的情况下，无论是FIFO还是LIFO，都要靠LinkedList来扛起大旗。
3. **总体性能ArrayList完胜。** 并不像传说的LinkedList更善于插入和删除动作。LinkedList的add(int index, <E> element)和remove(int index)方法，复杂度要实现O(1)的前提是，你必须提前获得操作元素的引用。不然LinkedList还是需要遍历到那个元素，最坏的情况下仍然是O(n)复杂度。StackOverFlow上关于这个问题的回答：[**《How is LinkedList's add(int, E) of O(1) complexity?》**](http://stackoverflow.com/questions/15732334/how-is-linkedlists-addint-e-of-o1-complexity)

这里附上1. ArrayList和LinkedList的几个重要操作的复杂度对比：
![ll_al](/thinkinginjava/uploads/tij4-11/ll_al.png)




#### ArrayList的实现

虽然说好了这章不细究具体实现。但还是忍不住看了一眼java.util.ArrayList的源代码。贴几个重要的片段上来，共赏。



```java


public class ArrayList<E> extends AbstractList<E>  implements List<E>, RandomAccess, Cloneable, java.io.Serializable {}


```


英雄先问出处，ArrayList继承自AbstractList<E>，是对List接口的一个通用实现。后面实现了4种接口，除了最重要的List<E>之外，Cloneable接口为了调用Object.clone()方法。其他两个RandomAccess和Serializable都是标记型接口，只为了语义功能，并没有定义任何内容。



```java


     /**
      * The array buffer into which the elements of the ArrayList are stored.
      * The capacity of the ArrayList is the length of this array buffer.
      */
     private transient Object[] elementData;	//ArrayList基于Array实现

     /**
      * The size of the ArrayList (the number of elements it contains).
      *
      * @serial
      */
     private int size;


```


接下来看成员字段。关键看到**Object[] elementData**，就知道ArrayList和传说中一样，是基于Array实现的。



```java


	/**
     * Constructs an empty list with the specified initial capacity.
     */
    public ArrayList(int initialCapacity) {
    super();
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal Capacity: "+
                                               initialCapacity);
    this.elementData = new Object[initialCapacity];
    }

    /**
     * Constructs an empty list with an initial capacity of ten.
     */
    public ArrayList() {
    this(10);	//默认长度10
    }

    /**
     * Constructs a list containing the elements of the specified
     * collection, in the order they are returned by the collection's
     * iterator.
     */
    public ArrayList(Collection<? extends E> c) {
    elementData = c.toArray();
    size = elementData.length;
    // c.toArray might (incorrectly) not return Object[] (see 6260652)
    if (elementData.getClass() != Object[].class)
        elementData = Arrays.copyOf(elementData, size, Object[].class);
    }


```


3种构造函数，对应于3种不同的声明方式。可以看到，ArrayList的默认初始大小是10。



```java


public boolean add(E e) {
    ensureCapacity(size + 1);  // Increments modCount!! 可变长度由这个方法实现
    elementData[size++] = e;
    return true;
}


```


这一段是add()方法。看到ensureCapacity(size + 1)这里，ArrayList的可变长度的功能就在这个方法里了。



```java


/**
     * Increases the capacity of this <tt>ArrayList</tt> instance, if
     * necessary, to ensure that it can hold at least the number of elements
     * specified by the minimum capacity argument.
     *
     * @param   minCapacity   the desired minimum capacity
     */
    public void ensureCapacity(int minCapacity) {
    modCount++;
    int oldCapacity = elementData.length;
    if (minCapacity > oldCapacity) {
        Object oldData[] = elementData;
        int newCapacity = (oldCapacity * 3)/2 + 1;	//关键算法：不够了长度就乘以1.5
            if (newCapacity < minCapacity)
        newCapacity = minCapacity;
            // minCapacity is usually close to size, so this is a win:
            elementData = Arrays.copyOf(elementData, newCapacity);
    }
    }


```


可变长度ensureCapacity()方法长这样，代码里一行关键的算法，透露了ArrayList的动态控制长度功能，是在长度不够的时候扩展为原来的1.5倍。不要小看这个简单的1.5倍。肯定是无数次实验的最优配置。智慧的结晶啊。

不能再写下去了，这没有底。以后专门写一个读JDK源代码的笔记吧。




##### LinkedList的实现

java.util.LinkedList的源代码也只贴几个重要的片段上来，共赏。先看一个LinkedList数据结构的简单图解，

![linkedlist](/thinkinginjava/uploads/tij4-11/linkedlist.jpg)
双链LinkedList的数据结构很简单，由一系列节点组成。每个节点包含一个数据储存单元，和两个指针，分别指向序列中的前一个节点，以及下一个节点。



```java


// 链表的表头，表头不包含任何数据。Entry是个链表类数据结构。
private transient Entry<E> header = new Entry<E>(null, null, null);

// LinkedList中元素个数
private transient int size = 0;


```


上面两行就能看出，LinkedList里的节点名叫Entry<E>。初始化的时候只有一个header节点。下面再看节点的数据结构，


```java


// 双向链表的节点所对应的数据结构。
// 包含3部分：上一节点，下一节点，当前节点值。
private static class Entry<E> {
// 当前节点所包含的值
E element;
// 下一个节点
Entry<E> next;
// 上一个节点
Entry<E> previous;

	/**
	* 链表节点的构造函数。
	* 参数说明：
	*   element  —— 节点所包含的数据
	*   next      —— 下一个节点
	*   previous —— 上一个节点
	*/
	Entry(E element, Entry<E> next, Entry<E> previous) {
    	this.element = element;
    	this.next = next;
    	this.previous = previous;
	}
}


```


也很简单，就像之前说的，中间一个element泛型数据本体，一前一后两个指向前一个和下一个节点的指针。



```java


// 将节点从链表中删除
private E remove(Entry<E> e) {
    if (e == header)
        throw new NoSuchElementException();

    E result = e.element;	//复制本体
    e.previous.next = e.next;	//让前一节点的指针跳过它，直接连到它的下一个节点。
	e.next.previous = e.previous;	//相反操作
	e.next = e.previous = null;	//消灭节点
	e.element = null;	//消灭节点
	size--;
	modCount++;
	return result;
}


```


LinkedList所谓效率胜过ArrayList的remove()方法，每次删除都分两步走：

* 给目标节点的前一结点，和后一节点牵线搭桥，架空目标节点

* 删除目标节点




#### 关于Map

已经说过，Map的特殊功用在于它为每个元素维护的一组“KEY-VALUE”对。Key用来搜索value值。选用Map的目的在于用户关心根据元素key搜寻元素value值的速度，而对元素在容器中的顺序不在乎。Map就相当于一个小型的搜索引擎。

Map手下最常用的是HashMap。它的 **get(Object key)** 方法效率最高。大多数情况下，能达到Array的 **O(1)** 的表现。我这里不想太深入，简单讲几点HashMap实现的大致原理，但实际上HashMap实现的每一步都是值得深究的。

![hashMap](/thinkinginjava/uploads/tij4-11/hashMap.png)
如上图，

* HashMap是由Array和LinkedList共同组成的。(Java 8当链表太长的时候，会自动变成红黑树。)

* 先开一个很大的Array数组(size=capacity)。

* 元素直接存放在以hash值为下标的Array槽内。

* 如果碰撞了，以链表的形式存在Array槽后；

**put(K key, V value)** 方法的步骤如下，

* 对key的hashCode()做hash，然后再计算index;（都是简单的位操作，常数复杂度 O(1)）

* 直接根据hash值来访问Array的下标。如果没碰撞直接放到bucket里；O(1)；

* 如果碰撞了，以链表的形式存在buckets后；往后插的过程是要遍历链表每个节点的。所以碰撞严重的话，对HashMap的性能影响严重。

* 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树；O(logn)；

* 如果节点已经存在就替换old value(保证key的唯一性)

* 如果bucket满了(超过load factor*current capacity)，就要resize。

综上所述，Java 8开始改成红黑树以后，一般情况下，put(K key, V value)动作是常数复杂度O(1)。最坏情况碰撞严重，最多O(logn)。还是可以接受的。

**get(Object key)** 方法的步骤如下，

* bucket里的第一个节点，直接命中，O(1)；

* 如果有冲突，则通过key.equals(k)去查找对应的entry

* 若为树，则在树中通过key.equals(k)查找，O(logn)；

* 若为链表，则在链表中通过key.equals(k)查找，O(n)。

综上所述，和插入put()方法一样，Java 8开始改成红黑树以后，一般情况下，搜索get(Object key)动作是常数复杂度O(1)。最坏情况碰撞严重，最多O(logn)。

看下面的HashMap的源码，可以清楚地看到HashMap也是基于Array实现复杂度为O(1)的快速随机访问。


```java


    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);

        // Find a power of 2 >= initialCapacity
        int capacity = 1;
        while (capacity < initialCapacity)
            capacity <<= 1;

        this.loadFactor = loadFactor;
        threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1);
        table = new Entry[capacity];	//这一行，证明HashMap也是基于Array实现复杂度为O(1)的快速随机访问
        useAltHashing = sun.misc.VM.isBooted() &&
                (capacity >= Holder.ALTERNATIVE_HASHING_THRESHOLD);
        init();
    }


```



下面这个是HashMap的hash算法。


```java


    final int hash(Object k) {
        int h = 0;
        if (useAltHashing) {
            if (k instanceof String) {
                return sun.misc.Hashing.stringHash32((String) k);
            }
            h = hashSeed;
        }

        h ^= k.hashCode();	//k的哈希值和哈希种子做异或运算（哈希种子为零，就相当于保留哈希值）
							//异或：相同则结果为0，不同则结果为1

        // This function ensures that hashCodes that differ only by
        // constant multiples at each bit position have a bounded
        // number of collisions (approximately 8 at default load factor).
        h ^= (h >>> 20) ^ (h >>> 12);	//h和他自己的右位移20位做异或，再和它的右位移12为做异或。
        return h ^ (h >>> 7) ^ (h >>> 4);	//h再和他自己的右位移7位做异或，再和它的右位移4为做异或。
    }


```


HashMap的Hash算法具体过程如下，

* 前面一个if先不管它。

* 通过hashCode()方法获取key的哈希值，和哈希种子做异或运算（哈希种子为零，就相当于保留哈希值）（hashCode()函数先当他黑箱）

* h和他自己的右位移20位做异或，再和它的右位移12为做异或。

* h再和他自己的右位移7位做异或，再和它的右位移4为做异或。

我还不太明白这算法背后的数学意义，但一点是肯定的，从java的hash()算法因为只是简单的位操作，复杂度是常数O(1)。

我另外一个好奇的事是这个Java每个对象自带的 **Object.hashCode()** 方法，到底是怎么返回哈希值的。毕竟HashMap的代码也都只是在系统给的哈希值的基础上做文章。StackOverFlow该问题的最高票答案[**《How is hashCode() calculated in Java?》**](http://stackoverflow.com/questions/2427631/how-is-hashcode-calculated-in-java)是这样说的：
> hashCode()是每个对象自带的方法。但几乎每个类都会重写这个方法。所以hashCode()的哈希值计算方法每种数据类型都是不同的。

下面举两个最常用的例子，int整型的哈希值等于它本身。


```java


   public int hashCode() {
       return value;
   }


```


String的哈希值算法稍微复杂一点，基本就是根据它自身的长度迭代，每次都乘以31再加上它的偏移值。


```java


   public int hashCode() {
       int h = hash;
       if (h == 0) {
           int off = offset;
           char val[] = value;
           int len = count;
		   //关键算法在这里
           for (int i = 0; i < len; i++) {
               h = 31*h + val[off++];
           }
           hash = h;
       }
       return h;
   }


```



对于HashMap具体的工作原理，会在以后的章节深入研究。下图总结一下HashMap的put(K key, V value)方法和get(Object key)方法的具体复杂度：
![hashMapComplexity](/thinkinginjava/uploads/tij4-11/hashMapComplexity.png)

最后，如果我们希望Map中元素以某种特殊的比较顺序排列，可以用**TreeMap**。如果希望元素保持插入时的顺序，可以用**LinkedHashMap**。但效率方面就要打一点折扣了。



##### 关于Set

前文说了Set是为了保持集合中元素的唯一性。和Map一样，同样有三种不同的实现：

* **HashSet**：非插入顺序，重点是get()搜索的高效。

* **TreeSet**：按元素的比较顺序排列。

* **LinkedSet**：保持插入顺序。

其中最常用的还是HashSet。TreeSet主要用于排序。排序算法取决于给定的**Comparator**。比如**String.CASE_INSENSITIVE_ORDER**.(按字母升序)


#####关于Queue

Queue接口有两个主要实现：

* **LinkedList**：不算Queue的嫡系，但却是最常用的。经常被用来面向Queue接口编程。

* **PriorityQueue**：和TreeSet一样，可以根据给出的Comparator维持特定的排序。



##### 关于Iterator迭代器和容器接口

设计Iterator迭代器的最重要原因不是为了迭代的效率。因为用普通的for语句迭代同样高效。

**Iterator的真正目的是为了让使用不同容器的代码之间可以去耦合。**

容器根据功能用途的不同，都被统一设计成了接口。Collection，List, Set, Map最常用的四大接口。和Iterator一样，为使用不同容器的代码去耦合才是设计这些接口的真正用意。

实际上，对于程序员自己开发的新容器而言，实现Iterator接口远比实现Collection接口要容易。因为Iterator接口只定义了hasNext()和next()两大方法。而Collection则复杂地多。因此Iterator才是简单去耦合的最佳方法。C++没有Collection接口，只有Iterator。

另外一个关于Iterator，我必须知道的是是：Java的foreach语法是面向Iterable接口的。任何类只要实现了Iterable接口都可以用foreach语法。foreach语法长这样，


```java


for(Element e : Collection<Element>){
	//do something
}


```


记住，所有Collection接口都实现了Iterable接口。但Maps和Array没有。



##### 关于过时的容器

书中作者提到的明确过时的容器有三个，这三个好汉分别是：

* **Vector**

* **HashTable**

* **Stack**



#### 关于Utilities

Utilities就是对于容器的一组实用静态方法。本身不需要实例化，专门用来操作对应类型的容器实例。
三大Utilities，分别用来专门操作各自的对应容器，

* **Collections**

* **Arrays**

* **Maps**

里面都是静态的方法。


